之前利用的优化器一直是'adam'这个优化器，默认学习率是0.001，其实adam是一种已经优化过的梯度下降算法，而且将学习率设置维0.001的这个优化器具有相当好的鲁棒性。
学习率其实就是梯度下降的速度，它是一个超参数，可以自己调整。可以根据具体的损失函数来调整学习率，尽快而且有效地寻找到最小值。
反向传播算法：之前在tf1.x中写过反向传播，其实反向传播算法就是根据过程中地输出，反馈回去，继续优化参数，直到达到要求输出结果。
优化器：常见地优化器：
    1、SGD（随机梯度下降优化器）：随机抽取样本，计算平均梯度值。
    2、RMSProp:应用于序列检测，应用在RNN中做文本识别。